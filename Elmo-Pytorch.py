# -*- coding: utf-8 -*-
"""elmo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17Ty3mRZcxauTcPPG1SOuemxKcxtODO3M

# Reading Dataset , Cleaning, Preprocessing
"""

import pandas as pd
import re
import nltk
import numpy as np
from collections import Counter
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import math  # Import the math module for perplexity calculation
import matplotlib.pyplot as plt
import torch.nn.functional as F

# Replace 'train.csv' and 'test.csv' with the actual file paths to your CSV files.
train_csv_file = 'train.csv'
test_csv_file = 'test.csv'

# Read the training data from train.csv
train_data = pd.read_csv(train_csv_file)

# Read the test data from test.csv
test_data = pd.read_csv(test_csv_file)

# Split the dataset into train and development sets.
dev_set_size = 7600
train_set = train_data.iloc[dev_set_size:]  # The portion after the development set.
dev_set = train_data.iloc[:dev_set_size]    # The first 7600 entries as the development set.

# Extract sentences and labels for train, dev, and test sets.
train_sentences = train_set['Description'].tolist()
train_labels = train_set['Class Index'].tolist()

dev_sentences = dev_set['Description'].tolist()
dev_labels = dev_set['Class Index'].tolist()

test_sentences = test_data['Description'].tolist()
test_labels = test_data['Class Index'].tolist()

# Function to clean a sentence
def clean_sentence(sentence):
    # Convert to lowercase
    sentence = sentence.lower()
    # Remove special characters, numbers, and extra whitespace
    sentence = re.sub(r'[^a-zA-Z\s]', '', sentence)
    # Tokenize the sentence (split into words)
    tokens = sentence.split()
    return ' '.join(tokens)

# Clean the sentences in the train, dev, and test sets
train_sentences_cleaned = [clean_sentence(sentence) for sentence in train_sentences]
dev_sentences_cleaned = [clean_sentence(sentence) for sentence in dev_sentences]
test_sentences_cleaned = [clean_sentence(sentence) for sentence in test_sentences]

# Tokenize the sentences using NLTK
nltk.download('punkt')
from nltk.tokenize import word_tokenize

train_sentences_tokenized = [word_tokenize(sentence) for sentence in train_sentences_cleaned]
dev_sentences_tokenized = [word_tokenize(sentence) for sentence in dev_sentences_cleaned]
test_sentences_tokenized = [word_tokenize(sentence) for sentence in test_sentences_cleaned]

# Define the maximum sequence length
max_seq_length = 256

# Function to pad sequences
def pad_sequences(sentences, max_length):
    padded_sentences = []
    for sentence in sentences:
        if len(sentence) >= max_length:
            padded_sentences.append(sentence[:max_length])
        else:
            padded_sentences.append(sentence + ['<PAD>'] * (max_length - len(sentence)))
    return padded_sentences

# Pad the sequences
train_sentences_padded = pad_sequences(train_sentences_tokenized, max_seq_length)
dev_sentences_padded = pad_sequences(dev_sentences_tokenized, max_seq_length)
test_sentences_padded = pad_sequences(test_sentences_tokenized, max_seq_length)

# Display the first few cleaned, tokenized, and padded sentences and labels for train, dev, and test sets.
print("Train Set Sentences (Padded):")
print(train_sentences_padded[:5])
print("Train Set Labels:")
print(train_labels[:5])

print("\nDevelopment Set Sentences (Padded):")
print(dev_sentences_padded[:5])
print("Development Set Labels:")
print(dev_labels[:5])

print("\nTest Set Sentences (Padded):")
print(test_sentences_padded[:5])
print("Test Set Labels:")
print(test_labels[:5])

print(len(train_sentences_cleaned))
print(len(dev_sentences_cleaned))
print(len(test_sentences_cleaned))

# Build vocabulary from the training set
all_words = [word for sentence in train_sentences_padded for word in sentence]
word_counts = Counter(all_words)

# Create word-to-index and index-to-word mappings
vocab = ['<UNK>'] + [word for word, count in word_counts.items()]
word2idx = {word: idx for idx, word in enumerate(vocab)}
idx2word = {idx: word for idx, word in enumerate(vocab)}

# Function to convert a sentence to indices
def sentence_to_indices(sentence, word2idx):
    return [word2idx.get(word, word2idx['<UNK>']) for word in sentence]

# Convert train, dev, and test sets to index sequences : Pretrain
train_indices = [sentence_to_indices(sentence, word2idx) for sentence in train_sentences_tokenized]
dev_indices = [sentence_to_indices(sentence, word2idx) for sentence in dev_sentences_tokenized]
test_indices = [sentence_to_indices(sentence, word2idx) for sentence in test_sentences_tokenized]

# Convert train, dev, and test sets to index sequences : Sentiment Analysis
train_indices_elmo = [sentence_to_indices(sentence, word2idx) for sentence in train_sentences_padded]
dev_indices_elmo = [sentence_to_indices(sentence, word2idx) for sentence in dev_sentences_padded]
test_indices_elmo = [sentence_to_indices(sentence, word2idx) for sentence in test_sentences_padded]


# Display the first few sentences and labels for train, dev, and test sets in index form
print("Train Set Sentences (Indices):")
print(train_indices[:2])  # Displaying the first 2 sentences for brevity
print("Train Set Labels:")
print(train_labels[:2])

print("\nDevelopment Set Sentences (Indices):")
print(dev_indices[:2])
print("Development Set Labels:")
print(dev_labels[:2])

print("\nTest Set Sentences (Indices):")
print(test_indices[:2])
print("Test Set Labels:")
print(test_labels[:2])

# Display the first few sentences and labels for train, dev, and test sets in index form
print("Train Set Sentences (Indices):")
print(train_indices_elmo[:2])  # Displaying the first 2 sentences for brevity
print("Train Set Labels:")
print(train_labels[:2])

print("\nDevelopment Set Sentences (Indices):")
print(dev_indices_elmo[:2])
print("Development Set Labels:")
print(dev_labels[:2])

print("\nTest Set Sentences (Indices):")
print(test_indices_elmo[:2])
print("Test Set Labels:")
print(test_labels[:2])

# Concatenate all sentences into a single sequence
train_all_sentences = [idx for sentence in train_indices for idx in sentence]
dev_all_sentences = [idx for sentence in dev_indices for idx in sentence]
test_all_sentences = [idx for sentence in test_indices for idx in sentence]

# Function to create n-grams from a sequence of indices
def create_ngrams(sequence, n):
    ngrams = []
    for i in range(len(sequence) - n + 1):
        ngram = sequence[i:i + n]
        ngrams.append(ngram)
    return torch.tensor(ngrams)

# Create n-grams for train, dev, and test datasets
ngram_size = 6

train_ngrams = create_ngrams(train_all_sentences, ngram_size)
dev_ngrams = create_ngrams(dev_all_sentences, ngram_size)
test_ngrams = create_ngrams(test_all_sentences, ngram_size)

# Convert n-grams to PyTorch tensors
train_data = torch.tensor(train_ngrams, dtype=torch.long)
dev_data = torch.tensor(dev_ngrams, dtype=torch.long)
test_data = torch.tensor(test_ngrams, dtype=torch.long)

# Create DataLoader instances
batch_size = 32

# train_loader = DataLoader(TensorDataset(train_data[:10000]), batch_size=batch_size, shuffle=False)
# dev_loader = DataLoader(TensorDataset(dev_data[:500]), batch_size=batch_size, shuffle=False)
# test_loader = DataLoader(TensorDataset(test_data[:500]), batch_size=batch_size, shuffle=False)

train_loader = DataLoader(TensorDataset(train_data[:100000]), batch_size=batch_size, shuffle=False)
dev_loader = DataLoader(TensorDataset(dev_data[:5000]), batch_size=batch_size, shuffle=False)
test_loader = DataLoader(TensorDataset(test_data[:5000]), batch_size=batch_size, shuffle=False)

# for batch in train_loader:
#     # 'batch' is a tensor containing a batch of data
#     print(batch)
#     break

import torch
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# Load GloVe word vectors
def load_glove_model(file_path):
    word_vectors = {}

    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            values = line.split()
            word = values[0]
            vector = [float(val) for val in values[1:]]
            word_vectors[word] = vector

    return word_vectors

glove_file_path = 'glove.6B.100d.txt'
glove_dict = load_glove_model(glove_file_path)

def create_embedding_matrix(glove_dict):
    '''
    Creates a weight matrix of the words that are common in the GloVe vocab and
    the dataset's vocab. Initializes OOV words with a zero vector.
    '''
    weights_matrix = torch.zeros((len(vocab), 100))
    words_found = 0
    for i, word in enumerate(vocab):
        try:
            weights_matrix[i] = torch.tensor(glove_dict[word])
            words_found += 1
        except:
            pass

    return weights_matrix, words_found

embedding_matrix, words_found = create_embedding_matrix(glove_dict)

words_found

class ELMo(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size, max_len, embedding_matrix, num_classes=4):
        super(ELMo, self).__init__()
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.batch_size = batch_size
        self.max_len = max_len
        self.num_classes = num_classes

        # Shared Embedding Layer
        self.embedding1 = nn.Embedding.from_pretrained(embedding_matrix)
        self.embedding2 = nn.Embedding.from_pretrained(embedding_matrix)

        # Forward Language Model
        self.lstm_forward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.lstm_forward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)

        # Backward Language Model
        self.lstm_backward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.lstm_backward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)

        # Linear layer for mode 1
        self.linear_mode1 = nn.Linear(200, vocab_size)

        # Linear layer for mode 2
        self.linear_mode2 = nn.Linear(200, vocab_size)

        # Define trainable weights for mode 0 as a vector of size 3 with values 0.33 each
        self.weights_mode0 = nn.Parameter(torch.tensor([0.33, 0.33, 0.33], dtype=torch.float32))

        self.final_linear = nn.Linear(self.max_len * self.embedding_dim * 2, num_classes)

    def forward(self, input_data, mode):
        if mode == 0:
            # Embedding for forward data
            forward_embed = self.embedding1(input_data)

            # Embedding for backward data
            backward_embed = self.embedding2(input_data)

            # Forward Language Model 1
            forward_lstm1, _ = self.lstm_forward1(forward_embed)

            # Backward Language Model 1
            backward_lstm1, _ = self.lstm_backward1(backward_embed)

            # Concatenate forward1 and backward1 outputs
            lstm_concat1 = torch.cat((forward_lstm1, backward_lstm1), dim=-1)

            # Forward Language Model 2
            forward_lstm2, _ = self.lstm_forward2(forward_lstm1)

            # Backward Language Model 2
            backward_lstm2, _ = self.lstm_backward2(backward_lstm1)

            # Concatenate forward2 and backward2 outputs
            lstm_concat2 = torch.cat((forward_lstm2, backward_lstm2), dim=-1)

            # Concatenate forward and backward embeddings word by word
            concatenated_embeddings = torch.cat((forward_embed, backward_embed), dim=-1)

            # Apply trainable weights for mode 0 to the concatenated tensors
            weighted_embeddings = concatenated_embeddings * self.weights_mode0[0]
            weighted_lstm1 = lstm_concat1 * self.weights_mode0[1]
            weighted_lstm2 = lstm_concat2 * self.weights_mode0[2]

            # Sum along the last dimension (600) to get a (32 x 256 x 200) tensor
            summed_tensor = weighted_embeddings + weighted_lstm1 + weighted_lstm2

            # Reshape the output to 32 x (256 x vocabSize)
            output = summed_tensor.view(-1, self.max_len * self.embedding_dim *2)

            # Additional linear layer to reduce the output to 32 x 4
            output = self.final_linear(output)

            # output = F.softmax(output, dim=1)

            return output

        elif mode == 1:
            # Embedding for forward data
            forward_embed = self.embedding1(input_data)

            # Forward Language Model 1
            forward_lstm1, _ = self.lstm_forward1(forward_embed, None) # Set batch_first to False

            # Forward Language Model 2
            forward_lstm2, _ = self.lstm_forward2(forward_lstm1, None) # Set batch_first to False

            # Concatenate forward_lstm1 and forward_lstm2 outputs
            lstm_concat = torch.cat((forward_lstm1, forward_lstm2), dim=-1)

            # Apply linear layer for mode 1
            output = self.linear_mode1(lstm_concat)

            return output

        elif mode == 2:
            # Embedding for backward data
            backward_embed = self.embedding2(input_data)

            # Backward Language Model 1
            backward_lstm1, _ = self.lstm_backward1(backward_embed, None) # Set batch_first to False

            # Backward Language Model 2
            backward_lstm2, _ = self.lstm_backward2(backward_lstm1, None) # Set batch_first to False

            # Concatenate backward_lstm1 and backward_lstm2 outputs
            lstm_concat = torch.cat((backward_lstm1, backward_lstm2), dim=-1)

            # Apply linear layer for mode 2
            output = self.linear_mode2(lstm_concat)

            return output

vocab_size = len(vocab)
embedding_dim = 100
hidden_dim = 100
max_len = 256
num_classes = 4

# Define the ELMo model
elmo = ELMo(vocab_size, embedding_dim, hidden_dim, batch_size, max_len, embedding_matrix, num_classes=num_classes)

# Move the model to the appropriate device (e.g., GPU)
elmo.to(device)

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(elmo.parameters(), lr=0.001)  # Adjust the learning rate as needed

# Training loop for mode 1 (next word prediction)
def train_mode1(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    total_tokens = 0  # Total tokens processed in the dataset

    for inputs in tqdm(train_loader):
        inputs = inputs[0]
        inputs = inputs.to(device)
        optimizer.zero_grad()

        # Use the first 5 words as input and the last 5 words as expected output
        input_seq = inputs[:, :5]
        target_seq = inputs[:, 1:]

        # Forward pass
        outputs = model(input_seq, mode=1)

        # Calculate the loss
        loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss
        total_loss += loss.item()

        # Calculate the total tokens processed
        total_tokens += target_seq.numel()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    # Calculate perplexity based on the total loss and tokens
    perplexity = math.exp(total_loss / total_tokens)

    avg_loss = total_loss / total_tokens

    return avg_loss

# Development loop
def eval_mode1(model, dev_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    total_tokens = 0  # Total tokens processed in the dataset

    with torch.no_grad():
        for inputs in tqdm(dev_loader):
            inputs = inputs[0]
            inputs = inputs.to(device)

            # Use the first 5 words as input and the last 5 words as expected output
            input_seq = inputs[:, :5]
            target_seq = inputs[:, 1:]

            # Forward pass
            outputs = model(input_seq, mode=1)

            # Calculate the loss
            loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss
            total_loss += loss.item()

            # Calculate the total tokens processed
            total_tokens += target_seq.numel()

    avg_loss = total_loss / total_tokens

    return avg_loss

# Testing loop
def test_mode1(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    total_tokens = 0  # Total tokens processed in the dataset

    with torch.no_grad():
        for inputs in tqdm(test_loader):
            inputs = inputs[0]
            inputs = inputs.to(device)

            # Use the first 5 words as input and the last 5 words as expected output
            input_seq = inputs[:, :5]
            target_seq = inputs[:, 1:]

            # Forward pass
            outputs = model(input_seq, mode=1)

            # Calculate the loss
            loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss
            total_loss += loss.item()

            # Calculate the total tokens processed
            total_tokens += target_seq.numel()

    avg_loss = total_loss / total_tokens

    return avg_loss

# Training loop for mode 2 (previous word prediction)
def train_mode2(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    total_tokens = 0  # Total tokens processed in the dataset

    for inputs in tqdm(train_loader):
        inputs = inputs[0]
        inputs = inputs.to(device)
        optimizer.zero_grad()

        # Reversing inputs
        inputs = torch.flip(inputs, dims=[1])

        # Use the first 5 words as input and the last 5 words as expected output
        input_seq = inputs[:, :5]
        target_seq = inputs[:, 1:]

        # Forward pass
        outputs = model(input_seq, mode=2)

        # Calculate the loss
        loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss
        total_loss += loss.item()

        # Calculate the total tokens processed
        total_tokens += target_seq.numel()

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    avg_loss = total_loss / total_tokens

    return avg_loss

# Development loop
def eval_mode2(model, dev_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    total_tokens = 0  # Total tokens processed in the dataset

    with torch.no_grad():
        for inputs in tqdm(dev_loader):
            inputs = inputs[0]
            inputs = inputs.to(device)

            # Reversing inputs
            inputs = torch.flip(inputs, dims=[1])

            # Use the first 5 words as input and the last 5 words as expected output
            input_seq = inputs[:, :5]
            target_seq = inputs[:, 1:]

            # Forward pass
            outputs = model(input_seq, mode=2)

            # Calculate the loss
            loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss
            total_loss += loss.item()

            # Calculate the total tokens processed
            total_tokens += target_seq.numel()

    avg_loss = total_loss / total_tokens

    return avg_loss

# Testing loop
def test_mode2(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    total_tokens = 0  # Total tokens processed in the dataset

    with torch.no_grad():
        for inputs in tqdm(test_loader):
            inputs = inputs[0]
            inputs = inputs.to(device)

            # Reversing inputs
            inputs = torch.flip(inputs, dims=[1])

            # Use the first 5 words as input and the last 5 words as expected output
            input_seq = inputs[:, :5]
            target_seq = inputs[:, 1:]

            # Forward pass
            outputs = model(input_seq, mode=2)

            # Calculate the loss
            loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss
            total_loss += loss.item()

            # Calculate the total tokens processed
            total_tokens += target_seq.numel()

    avg_loss = total_loss / total_tokens

    return avg_loss

# Training hyperparameters
num_epochs = 15  # Adjust as needed

# Initialize empty lists to store loss and perplexity values
loss_mode1_values = []
perplexity_mode1_values = []
loss_mode2_values = []
perplexity_mode2_values = []
dev_loss_mode1_values = []  # To store dev loss
dev_perplexity_mode1_values = []  # To store dev perplexity
dev_loss_mode2_values = []  # To store dev loss
dev_perplexity_mode2_values = []  # To store dev perplexity
test_loss_mode1_values = []  # To store test loss
test_perplexity_mode1_values = []  # To store test perplexity
test_loss_mode2_values = []  # To store test loss
test_perplexity_mode2_values = []  # To store test perplexity

# Training loop for mode 1
for epoch in range(num_epochs):
    train_loss_mode1 = train_mode1(elmo, train_loader, optimizer, criterion, device)
    loss_mode1_values.append(train_loss_mode1)

    # Calculate perplexity based on the loss (assuming cross-entropy loss)
    perplexity_mode1 = math.exp(train_loss_mode1)
    perplexity_mode1_values.append(perplexity_mode1)

    print(f"Epoch {epoch+1}/{num_epochs} (Mode 1) - Train Loss: {train_loss_mode1:.4f}, Train Perplexity: {perplexity_mode1:.4f}")

    # Evaluate on dev dataset
    dev_loss_mode1 = eval_mode1(elmo, dev_loader, criterion, device)
    dev_loss_mode1_values.append(dev_loss_mode1)

    # Calculate perplexity based on the dev loss (assuming cross-entropy loss)
    dev_perplexity_mode1 = math.exp(dev_loss_mode1)
    dev_perplexity_mode1_values.append(dev_perplexity_mode1)

    print(f"Epoch {epoch+1}/{num_epochs} (Mode 1) - Dev Loss: {dev_loss_mode1:.4f}, Dev Perplexity: {dev_perplexity_mode1:.4f}")

# Training loop for mode 2
for epoch in range(num_epochs):
    train_loss_mode2 = train_mode2(elmo, train_loader, optimizer, criterion, device)
    loss_mode2_values.append(train_loss_mode2)

    # Calculate perplexity based on the loss (assuming cross-entropy loss)
    perplexity_mode2 = math.exp(train_loss_mode2)
    perplexity_mode2_values.append(perplexity_mode2)

    print(f"Epoch {epoch+1}/{num_epochs} (Mode 2) - Train Loss: {train_loss_mode2:.4f}, Train Perplexity: {perplexity_mode2:.4f}")

    # Evaluate on dev dataset
    dev_loss_mode2 = eval_mode2(elmo, dev_loader, criterion, device)
    dev_loss_mode2_values.append(dev_loss_mode2)

    # Calculate perplexity based on the dev loss (assuming cross-entropy loss)
    dev_perplexity_mode2 = math.exp(dev_loss_mode2)
    dev_perplexity_mode2_values.append(dev_perplexity_mode2)

    print(f"Epoch {epoch+1}/{num_epochs} (Mode 2) - Dev Loss: {dev_loss_mode2:.4f}, Dev Perplexity: {dev_perplexity_mode2:.4f}")

# Testing loop
test_loss_mode1 = test_mode1(elmo, test_loader, criterion, device)
test_loss_mode2 = test_mode2(elmo, test_loader, criterion, device)

# Calculate perplexity based on the test loss (assuming cross-entropy loss)
test_perplexity_mode1 = math.exp(test_loss_mode1)
test_perplexity_mode2 = math.exp(test_loss_mode2)

# Store test results
test_loss_mode1_values.append(test_loss_mode1)
test_perplexity_mode1_values.append(test_perplexity_mode1)
test_loss_mode2_values.append(test_loss_mode2)
test_perplexity_mode2_values.append(test_perplexity_mode2)

print(f"Test Loss (Mode 1): {test_loss_mode1:.4f}, Test Perplexity (Mode 1): {test_perplexity_mode1:.4f}")
print(f"Test Loss (Mode 2): {test_loss_mode2:.4f}, Test Perplexity (Mode 2): {test_perplexity_mode2:.4f}")

# Plotting loss and perplexity graphs
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))

# Loss plot for Mode 1
plt.subplot(2, 2, 1)
plt.plot(range(num_epochs), loss_mode1_values, label="Train Loss (Mode 1)")
plt.plot(range(num_epochs), dev_loss_mode1_values, label="Dev Loss (Mode 1)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss (Mode 1)")
plt.legend()

# Perplexity plot for Mode 1
plt.subplot(2, 2, 2)
plt.plot(range(num_epochs), perplexity_mode1_values, label="Train Perplexity (Mode 1)")
plt.plot(range(num_epochs), dev_perplexity_mode1_values, label="Dev Perplexity (Mode 1)")
plt.xlabel("Epoch")
plt.ylabel("Perplexity")
plt.title("Perplexity (Mode 1)")
plt.legend()

# Loss plot for Mode 2
plt.subplot(2, 2, 3)
plt.plot(range(num_epochs), loss_mode2_values, label="Train Loss (Mode 2)")
plt.plot(range(num_epochs), dev_loss_mode2_values, label="Dev Loss (Mode 2)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss (Mode 2)")
plt.legend()

# Perplexity plot for Mode 2
plt.subplot(2, 2, 4)
plt.plot(range(num_epochs), perplexity_mode2_values, label="Train Perplexity (Mode 2)")
plt.plot(range(num_epochs), dev_perplexity_mode2_values, label="Dev Perplexity (Mode 2)")
plt.xlabel("Epoch")
plt.ylabel("Perplexity")
plt.title("Perplexity (Mode 2)")
plt.legend()

plt.tight_layout()
plt.show()

# Display test results
print(f"Test Loss (Mode 1): {test_loss_mode1:.4f}, Test Perplexity (Mode 1): {test_perplexity_mode1:.4f}")
print(f"Test Loss (Mode 2): {test_loss_mode2:.4f}, Test Perplexity (Mode 2): {test_perplexity_mode2:.4f}")

"""# Saving Pretrained Model"""

# Save the model to a file
torch.save(elmo.state_dict(), 'elmo_Pretrain.pth')

"""# Loading Pretrained Parameters"""

# Load the pretrained model state
elmo.load_state_dict(torch.load('elmo_model.pth', map_location=device))  # 'device' should be set to the appropriate device

# Move the loaded model to the appropriate device
elmo.to(device)

"""# Some Preprocessing"""

# Convert n-grams to PyTorch tensors
train_data_elmo = torch.tensor(train_indices_elmo, dtype=torch.long)
dev_data_elmo = torch.tensor(dev_indices_elmo, dtype=torch.long)
test_data_elmo = torch.tensor(test_indices_elmo, dtype=torch.long)

# # Combine data with labels
# train_data_with_labels = TensorDataset(train_data_elmo[:10000], torch.tensor(train_labels[:10000], dtype=torch.long))
# dev_data_with_labels = TensorDataset(dev_data_elmo[:500], torch.tensor(dev_labels[:500], dtype=torch.long))
# test_data_with_labels = TensorDataset(test_data_elmo[:500], torch.tensor(test_labels[:500], dtype=torch.long))

# Combine data with labels
train_data_with_labels = TensorDataset(train_data_elmo[:100000], torch.tensor(train_labels[:100000], dtype=torch.long))
dev_data_with_labels = TensorDataset(dev_data_elmo[:5000], torch.tensor(dev_labels[:5000], dtype=torch.long))
test_data_with_labels = TensorDataset(test_data_elmo[:5000], torch.tensor(test_labels[:5000], dtype=torch.long))

# Create DataLoader instances
batch_size = 32

train_loader = DataLoader(train_data_with_labels, batch_size=batch_size, shuffle=False)
dev_loader = DataLoader(dev_data_with_labels, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_data_with_labels, batch_size=batch_size, shuffle=False)

# Display the first few sentences and labels for train, dev, and test sets in index form
print("Train Set Sentences (Indices):")
print(train_indices_elmo[:2])  # Displaying the first 2 sentences for brevity
print("Train Set Labels:")
print(train_labels[:2])

print("\nDevelopment Set Sentences (Indices):")
print(dev_indices_elmo[:2])
print("Development Set Labels:")
print(dev_labels[:2])

print("\nTest Set Sentences (Indices):")
print(test_indices_elmo[:2])
print("Test Set Labels:")
print(test_labels[:2])

# Training loop for mode 0 (4-class classification) on GPU
def train_elmo(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0.0
    total_correct = 0
    all_predicted = []
    all_actual = []

    for inputs, target_seq in tqdm(train_loader):
        inputs = inputs.to(device)
        target_seq = target_seq.to(device)
        optimizer.zero_grad()

        input_seq = inputs

        # Forward pass
        outputs = model(input_seq, mode=0)

        target_seq = target_seq - 1

        # Calculate the loss directly with integer class labels
        loss = criterion(outputs, target_seq)
        total_loss += loss.item()

        # Calculate correct predictions
        predicted_classes = torch.argmax(outputs, dim=1)
        correct_predictions = torch.sum(torch.eq(predicted_classes, target_seq))
        total_correct += correct_predictions.item()

        all_predicted.extend(predicted_classes.cpu().numpy())
        all_actual.extend(target_seq.cpu().numpy())

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

    accuracy = total_correct / len(all_actual)  # Calculate accuracy based on the entire dataset

    return total_loss / len(train_loader), accuracy, all_predicted, all_actual

# Evaluation loop for mode 0 (4-class classification) on GPU
def evaluate_elmo(model, dev_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    all_predicted = []
    all_actual = []

    with torch.no_grad():
        for inputs, target_seq in tqdm(dev_loader):
            inputs = inputs.to(device)
            target_seq = target_seq.to(device)

            input_seq = inputs

            # Forward pass
            outputs = model(input_seq, mode=0)

            target_seq = target_seq - 1

            # Calculate the loss directly with integer class labels
            loss = criterion(outputs, target_seq)
            total_loss += loss.item()

            # Calculate correct predictions
            predicted_classes = torch.argmax(outputs, dim=1)
            correct_predictions = torch.sum(torch.eq(predicted_classes, target_seq))
            total_correct += correct_predictions.item()

            all_predicted.extend(predicted_classes.cpu().numpy())
            all_actual.extend(target_seq.cpu().numpy())

    accuracy = total_correct / len(all_actual)  # Calculate accuracy based on the entire dataset

    return total_loss / len(dev_loader), accuracy, all_predicted, all_actual


# Test loop for mode 0 (4-class classification) on GPU
def evaluate_elmo(model, test_loader, criterion, device):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    all_predicted = []
    all_actual = []

    with torch.no_grad():
        for inputs, target_seq in tqdm(test_loader):
            inputs = inputs.to(device)
            target_seq = target_seq.to(device)

            input_seq = inputs

            # Forward pass
            outputs = model(input_seq, mode=0)

            target_seq = target_seq - 1

            # Calculate the loss directly with integer class labels
            loss = criterion(outputs, target_seq)
            total_loss += loss.item()

            # Calculate correct predictions
            predicted_classes = torch.argmax(outputs, dim=1)
            correct_predictions = torch.sum(torch.eq(predicted_classes, target_seq))
            total_correct += correct_predictions.item()

            all_predicted.extend(predicted_classes.cpu().numpy())
            all_actual.extend(target_seq.cpu().numpy())

    accuracy = total_correct / len(all_actual)  # Calculate accuracy based on the entire dataset

    return total_loss / len(test_loader), accuracy, all_predicted, all_actual

# Training hyperparameters
num_epochs = 15

# Initialize lists to store evaluation metrics for training, dev, and test
train_accuracy_values = []
train_precision_values = []
train_recall_values = []
train_micro_f1_values = []

dev_accuracy_values = []
dev_precision_values = []
dev_recall_values = []
dev_micro_f1_values = []

test_accuracy_values = []
test_precision_values = []
test_recall_values = []
test_micro_f1_values = []

# Initialize variables to store predicted and actual values
train_all_predicted = []
train_all_actual = []

dev_all_predicted = []
dev_all_actual = []

test_all_predicted = []
test_all_actual = []

# Define the loss function and optimizer
# criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(elmo.parameters(), lr=0.001)

# Training loop
for epoch in range(num_epochs):
    # Training
    train_loss, train_accuracy, train_predicted, train_actual = train_elmo(elmo, train_loader, optimizer, criterion, device)

    # Calculate precision, recall, and micro F1 score for training
    train_precision = precision_score(train_actual, train_predicted, average='weighted')
    train_recall = recall_score(train_actual, train_predicted, average='weighted')
    train_micro_f1 = f1_score(train_actual, train_predicted, average='micro')

    # Store training evaluation metrics
    train_accuracy_values.append(train_accuracy)
    train_precision_values.append(train_precision)
    train_recall_values.append(train_recall)
    train_micro_f1_values.append(train_micro_f1)

    train_all_predicted.extend(train_predicted)
    train_all_actual.extend(train_actual)

    print(f"Epoch {epoch+1}/{num_epochs} (Train) - Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2%}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, Micro F1 Score: {train_micro_f1:.4f}")

    # Validation on dev_loader
    dev_loss, dev_accuracy, dev_predicted, dev_actual = evaluate_elmo(elmo, dev_loader, criterion, device)

    # Calculate precision, recall, and micro F1 score for dev_loader
    dev_precision = precision_score(dev_actual, dev_predicted, average='weighted')
    dev_recall = recall_score(dev_actual, dev_predicted, average='weighted')
    dev_micro_f1 = f1_score(dev_actual, dev_predicted, average='micro')

    # Store dev evaluation metrics per epoch
    dev_accuracy_values.append(dev_accuracy)
    dev_precision_values.append(dev_precision)
    dev_recall_values.append(dev_recall)
    dev_micro_f1_values.append(dev_micro_f1)

    dev_all_predicted.extend(dev_predicted)
    dev_all_actual.extend(dev_actual)

    print(f"Epoch {epoch+1}/{num_epochs} (Dev) - Loss: {dev_loss:.4f}, Accuracy: {dev_accuracy:.2%}, Precision: {dev_precision:.4f}, Recall: {dev_recall:.4f}, Micro F1 Score: {dev_micro_f1:.4f}")

# Testing on test_loader after training
test_loss, test_accuracy, test_predicted, test_actual = evaluate_elmo(elmo, test_loader, criterion, device)

# Calculate precision, recall, and micro F1 score for test_loader
test_precision = precision_score(test_actual, test_predicted, average='weighted')
test_recall = recall_score(test_actual, test_predicted, average='weighted')
test_micro_f1 = f1_score(test_actual, test_predicted, average='micro')

# Store test evaluation metrics
test_accuracy_values.append(test_accuracy)
test_precision_values.append(test_precision)
test_recall_values.append(test_recall)
test_micro_f1_values.append(test_micro_f1)

test_all_predicted.extend(test_predicted)
test_all_actual.extend(test_actual)

print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2%}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, Micro F1 Score: {test_micro_f1:.4f}")

# Plot evaluation metrics
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 18))

# Accuracy plot
plt.subplot(3, 2, 1)
plt.plot(range(num_epochs), train_accuracy_values, label="Train Accuracy")
plt.plot(range(num_epochs), dev_accuracy_values, label="Dev Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Accuracy")
plt.legend()

# Precision plot
plt.subplot(3, 2, 2)
plt.plot(range(num_epochs), train_precision_values, label="Train Precision")
plt.plot(range(num_epochs), dev_precision_values, label="Dev Precision")
plt.xlabel("Epoch")
plt.ylabel("Precision")
plt.title("Precision")
plt.legend()

# Recall plot
plt.subplot(3, 2, 3)
plt.plot(range(num_epochs), train_recall_values, label="Train Recall")
plt.plot(range(num_epochs), dev_recall_values, label="Dev Recall")
plt.xlabel("Epoch")
plt.ylabel("Recall")
plt.title("Recall")
plt.legend()

# Micro F1 Score plot
plt.subplot(3, 2, 4)
plt.plot(range(num_epochs), train_micro_f1_values, label="Train Micro F1 Score")
plt.plot(range(num_epochs), dev_micro_f1_values, label="Dev Micro F1 Score")
plt.xlabel("Epoch")
plt.ylabel("Micro F1 Score")
plt.title("Micro F1 Score")
plt.legend()

# Confusion Matrix for Test Data
conf_matrix = confusion_matrix(test_all_actual, test_all_predicted)
plt.subplot(3, 2, 5)
plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
plt.title('Test Confusion Matrix')
plt.colorbar()
tick_marks = np.arange(len(conf_matrix))
plt.xticks(tick_marks, range(1, 5), rotation=45)
plt.yticks(tick_marks, range(1, 5))
plt.xlabel('Predicted')
plt.ylabel('Actual')

plt.tight_layout()
plt.show()

"""# Saving the Model"""

# Save the model to a file
torch.save(elmo.state_dict(), 'elmo_model.pth')

"""# Loading the Model"""

# Create an instance of the ELMo model
loaded_elmo = ELMo(vocab_size, embedding_dim, hidden_dim, batch_size, max_len, embedding_matrix, num_classes=num_classes)

# Load the pretrained model state
loaded_elmo.load_state_dict(torch.load('elmo_model.pth', map_location=device))  # 'device' should be set to the appropriate device

# Move the loaded model to the appropriate device
loaded_elmo.to(device)

print(loaded_elmo.weights_mode0)

# Define a list of weight vectors
weight_vectors = [
    [0.0972, 0.1431, 0.1445],
    [0.33, 0.33, 0.33],
    [0, 0.5, 0.5],
    [0.5, 0.25, 0.25],
    [0.1, 0.2, 0.7],
    [0.4, 0.4, 0.2],
    [0.6, 0.2, 0.2],
    [0.1, 0.3, 0.6],
    [0.15, 0.15, 0.7],
    [0.1, 0.7, 0.2]
]

# Initialize lists to store evaluation metrics
accuracy_values = []
precision_values = []
recall_values = []
micro_f1_values = []

# Initialize variables to store predicted and actual values
all_predicted = []
all_actual = []

# Initialize a subplot counter for confusion matrices
subplot_counter = 1

# Create a figure for confusion matrices and evaluation metrics with increased height
plt.figure(figsize=(16, 24))  # Increased height

# Iterate through each weight vector
for weights in weight_vectors:
    # Create a torch.nn.Parameter from the weight vector and set it in the model
    weight_parameter = nn.Parameter(torch.tensor(weights, device=device, dtype=torch.float32, requires_grad=True))
    loaded_elmo.weights_mode0 = weight_parameter

    # Testing on test_loader after training
    test_loss, test_accuracy, test_predicted, test_actual = evaluate_elmo(loaded_elmo, test_loader, criterion, device)

    # Calculate precision, recall, and micro F1 score for test_loader
    test_precision = precision_score(test_actual, test_predicted, average='weighted')
    test_recall = recall_score(test_actual, test_predicted, average='weighted')
    test_micro_f1 = f1_score(test_actual, test_predicted, average='micro')

    # Store test evaluation metrics
    accuracy_values.append(test_accuracy)
    precision_values.append(test_precision)
    recall_values.append(test_recall)
    micro_f1_values.append(test_micro_f1)

    # Store predicted and actual values for confusion matrix
    all_predicted.extend(test_predicted)
    all_actual.extend(test_actual)

    # Print evaluation metrics for the current weight vector
    print(f"Weight Vector: {weights}")
    print(f"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2%}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, Micro F1 Score: {test_micro_f1:.4f}")

    # Calculate the confusion matrix
    conf_matrix = confusion_matrix(test_actual, test_predicted)

    # Create a subplot for the confusion matrix with increased height
    plt.subplot(len(weight_vectors), 2, subplot_counter)
    subplot_counter += 1

    # Display confusion matrix
    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title(f'Confusion Matrix (Weight Vector: {weights})')
    plt.colorbar()
    tick_marks = np.arange(len(weights))
    plt.xticks(tick_marks, range(1, len(weights) + 1), rotation=45)  # Adjust the labels based on the number of classes
    plt.yticks(tick_marks, range(1, len(weights) + 1))  # Adjust the labels based on the number of classes
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

# Plot the evaluation metrics (accuracy, micro F1, precision, and recall) with increased height
plt.subplot(len(weight_vectors), 2, subplot_counter)
subplot_counter += 1

# Accuracy plot
plt.plot(range(len(weight_vectors)), accuracy_values, marker='o')
plt.xlabel("Weight Vector")
plt.ylabel("Accuracy")
plt.title("Accuracy")

# Micro F1 Score plot
plt.subplot(len(weight_vectors), 2, subplot_counter)
subplot_counter += 1
plt.plot(range(len(weight_vectors)), micro_f1_values, marker='o')
plt.xlabel("Weight Vector")
plt.ylabel("Micro F1 Score")
plt.title("Micro F1 Score")

# Precision plot
plt.subplot(len(weight_vectors), 2, subplot_counter)
subplot_counter += 1
plt.plot(range(len(weight_vectors)), precision_values, marker='o')
plt.xlabel("Weight Vector")
plt.ylabel("Precision")
plt.title("Precision")

# Recall plot
plt.subplot(len(weight_vectors), 2, subplot_counter)
plt.plot(range(len(weight_vectors)), recall_values, marker='o')
plt.xlabel("Weight Vector")
plt.ylabel("Recall")
plt.title("Recall")

plt.tight_layout()
plt.show()

