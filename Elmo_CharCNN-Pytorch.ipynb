{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Reading Dataset , Cleaning, Preprocessing"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/yash/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import pandas as pd\n","import re\n","import nltk\n","import numpy as np\n","from collections import Counter\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm\n","import math  # Import the math module for perplexity calculation\n","import matplotlib.pyplot as plt \n","import torch.nn.functional as F\n","\n","# Replace 'train.csv' and 'test.csv' with the actual file paths to your CSV files.\n","train_csv_file = 'train.csv'\n","test_csv_file = 'test.csv'\n","\n","# Read the training data from train.csv\n","train_data = pd.read_csv(train_csv_file)\n","\n","# Read the test data from test.csv\n","test_data = pd.read_csv(test_csv_file)\n","\n","# Split the dataset into train and development sets.\n","dev_set_size = 7600\n","train_set = train_data.iloc[dev_set_size:]  # The portion after the development set.\n","dev_set = train_data.iloc[:dev_set_size]    # The first 7600 entries as the development set.\n","\n","# Extract sentences and labels for train, dev, and test sets.\n","train_sentences = train_set['Description'].tolist()\n","train_labels = train_set['Class Index'].tolist()\n","\n","dev_sentences = dev_set['Description'].tolist()\n","dev_labels = dev_set['Class Index'].tolist()\n","\n","test_sentences = test_data['Description'].tolist()\n","test_labels = test_data['Class Index'].tolist()\n","\n","# Function to clean a sentence\n","def clean_sentence(sentence):\n","    # Convert to lowercase\n","    sentence = sentence.lower()\n","    # Remove special characters, numbers, and extra whitespace\n","    sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)\n","    # Tokenize the sentence (split into words)\n","    tokens = sentence.split()\n","    return ' '.join(tokens)\n","\n","# Clean the sentences in the train, dev, and test sets\n","train_sentences_cleaned = [clean_sentence(sentence) for sentence in train_sentences]\n","dev_sentences_cleaned = [clean_sentence(sentence) for sentence in dev_sentences]\n","test_sentences_cleaned = [clean_sentence(sentence) for sentence in test_sentences]\n","\n","# Tokenize the sentences using NLTK\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","train_sentences_tokenized = [word_tokenize(sentence) for sentence in train_sentences_cleaned]\n","dev_sentences_tokenized = [word_tokenize(sentence) for sentence in dev_sentences_cleaned]\n","test_sentences_tokenized = [word_tokenize(sentence) for sentence in test_sentences_cleaned]\n"]},{"cell_type":"markdown","metadata":{},"source":["# Char Preprocessing (For ELMO)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["train_char_sentences = [[list(word) for word in sentence] for sentence in train_sentences_tokenized]\n","dev_char_sentences = [[list(word) for word in sentence] for sentence in dev_sentences_tokenized]\n","test_char_sentences = [[list(word) for word in sentence] for sentence in test_sentences_tokenized]"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Define the desired sequence lengths\n","max_sentence_length = 128\n","max_word_length = 64  # Or your desired maximum word length\n","\n","# Create character-level vocabulary and assign indices\n","char_to_idx = {'<PAD>': 0, '<UNK>': 1}  # Add padding and unknown tokens\n","char_vocab_size = len(char_to_idx)\n","char_embedding_dim = 50  # Set an appropriate embedding dimension\n","\n","# Function to pad or truncate a list to a fixed length\n","def pad_or_truncate(lst, length, padding_value):\n","    if len(lst) >= length:\n","        return lst[:length]\n","    else:\n","        return lst + [padding_value] * (length - len(lst))\n","\n","# Convert characters to indices for training data\n","char_indices_train = [\n","    [\n","        pad_or_truncate(\n","            [char_to_idx.get(char, 1) for char in word],\n","            max_word_length,\n","            0\n","        )\n","        for word in sentence\n","    ]\n","    for sentence in train_char_sentences\n","]\n","\n","# Convert characters to indices for development data\n","char_indices_dev = [\n","    [\n","        pad_or_truncate(\n","            [char_to_idx.get(char, 1) for char in word],\n","            max_word_length,\n","            0\n","        )\n","        for word in sentence\n","    ]\n","    for sentence in dev_char_sentences\n","]\n","\n","# Convert characters to indices for test data\n","char_indices_test = [\n","    [\n","        pad_or_truncate(\n","            [char_to_idx.get(char, 1) for char in word],\n","            max_word_length,\n","            0\n","        )\n","        for word in sentence\n","    ]\n","    for sentence in test_char_sentences\n","]\n","\n","# Pad or truncate character sequences for training data\n","char_indices_train = [\n","    pad_or_truncate(sentence, max_sentence_length, [0] * max_word_length)\n","    for sentence in char_indices_train\n","]\n","\n","# Pad or truncate character sequences for development data\n","char_indices_dev = [\n","    pad_or_truncate(sentence, max_sentence_length, [0] * max_word_length)\n","    for sentence in char_indices_dev\n","]\n","\n","# Pad or truncate character sequences for test data\n","char_indices_test = [\n","    pad_or_truncate(sentence, max_sentence_length, [0] * max_word_length)\n","    for sentence in char_indices_test\n","]\n","\n","# Convert to PyTorch tensors\n","char_input_data_train = torch.tensor(char_indices_train)\n","char_input_data_dev = torch.tensor(char_indices_dev)\n","char_input_data_test = torch.tensor(char_indices_test)\n","\n","\n","# Create DataLoader instances\n","batch_size = 32\n","\n","train_loader_elmo = DataLoader(TensorDataset(char_input_data_train[:10000]), batch_size=batch_size, shuffle=False)\n","dev_loader_elmo = DataLoader(TensorDataset(char_input_data_dev[:500]), batch_size=batch_size, shuffle=False)\n","test_loader_elmo = DataLoader(TensorDataset(char_input_data_test[:500]), batch_size=batch_size, shuffle=False)\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([112400, 128, 64])\n","torch.Size([7600, 128, 64])\n","torch.Size([7600, 128, 64])\n"]}],"source":["print(char_input_data_train.shape)\n","print(char_input_data_dev.shape)\n","print(char_input_data_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Char Preprocessing (For Language Modelling)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Convert it to a list of words where each word is a list of characters\n","train_char_sentences_flattened = [char_list for word_list in train_char_sentences for char_list in word_list]\n","dev_char_sentences_flattened = [char_list for word_list in dev_char_sentences for char_list in word_list]\n","test_char_sentences_flattened = [char_list for word_list in test_char_sentences for char_list in word_list]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Convert characters to indices for training data\n","char_indices_train_flattened = [\n","    pad_or_truncate(\n","        [char_to_idx.get(char, 1) for char in word],\n","        max_word_length,\n","        0\n","    )\n","    for sentence in train_char_sentences_flattened\n","    for word in sentence\n","]\n","\n","# Convert characters to indices for development data\n","char_indices_dev_flattened = [\n","    pad_or_truncate(\n","        [char_to_idx.get(char, 1) for char in word],\n","        max_word_length,\n","        0\n","    )\n","    for sentence in dev_char_sentences_flattened\n","    for word in sentence\n","]\n","\n","# Convert characters to indices for test data\n","char_indices_test_flattened = [\n","    pad_or_truncate(\n","        [char_to_idx.get(char, 1) for char in word],\n","        max_word_length,\n","        0\n","    )\n","    for sentence in test_char_sentences_flattened\n","    for word in sentence\n","]"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Convert to PyTorch tensors\n","char_input_data_train_flattened = torch.tensor(char_indices_train_flattened)\n","char_input_data_dev_flattened = torch.tensor(char_indices_dev_flattened)\n","char_input_data_test_flattened = torch.tensor(char_indices_test_flattened)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([17307370, 64])\n","torch.Size([1213019, 64])\n","torch.Size([1167836, 64])\n"]}],"source":["print(char_input_data_train_flattened.shape)\n","print(char_input_data_dev_flattened.shape)\n","print(char_input_data_test_flattened.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Create DataLoader instances\n","batch_size = 32\n","\n","train_loader = DataLoader(TensorDataset(char_input_data_train_flattened[:10000]), batch_size=batch_size, shuffle=False)\n","dev_loader = DataLoader(TensorDataset(char_input_data_dev_flattened[:500]), batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(TensorDataset(char_input_data_test_flattened[:500]), batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-04-24T00:25:44.875848Z","iopub.status.busy":"2023-04-24T00:25:44.875154Z","iopub.status.idle":"2023-04-24T00:25:44.883771Z","shell.execute_reply":"2023-04-24T00:25:44.882461Z","shell.execute_reply.started":"2023-04-24T00:25:44.875790Z"},"trusted":true},"outputs":[],"source":["import torch\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-04-24T00:28:40.850651Z","iopub.status.busy":"2023-04-24T00:28:40.849804Z","iopub.status.idle":"2023-04-24T00:28:40.892903Z","shell.execute_reply":"2023-04-24T00:28:40.891959Z","shell.execute_reply.started":"2023-04-24T00:28:40.850595Z"},"trusted":true},"outputs":[],"source":["class CharCNN(nn.Module):\n","    def __init__(self, char_vocab_size, char_embedding_dim, num_filters, kernel_sizes):\n","        super(CharCNN, self).__init__()\n","        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n","        self.conv_layers = nn.ModuleList([\n","            nn.Conv1d(char_embedding_dim, num_filters, kernel_size) for kernel_size in kernel_sizes\n","        ])\n","\n","    def forward(self, char_input):\n","        char_embedded = self.char_embedding(char_input)  # [batch_size, max_word_len, char_embedding_dim]\n","        char_embedded = char_embedded.permute(0, 2, 1)  # [batch_size, char_embedding_dim, max_word_len]\n","\n","        conv_outputs = [conv(char_embedded) for conv in self.conv_layers]  # Apply convolutional layers\n","        pooled_outputs = [torch.max(conv_output, dim=2)[0] for conv_output in conv_outputs]  # Max-pooling\n","        char_cnn_output = torch.cat(pooled_outputs, dim=1)  # Concatenate pooled outputs\n","        return char_cnn_output\n","    \n","\n","class ELMo(nn.Module):\n","    def __init__(self, hidden_dim, batch_size, max_len, char_cnn1=None, char_cnn2=None, num_classes=4):\n","        super(ELMo, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.batch_size = batch_size\n","        self.max_len = max_len\n","        self.num_classes = num_classes\n","\n","        # Character CNN Embedding\n","        self.char_cnn1 = char_cnn1\n","        self.char_cnn1 = char_cnn2\n","\n","        # Forward Language Model\n","        self.lstm_forward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.lstm_forward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","\n","        # Backward Language Model\n","        self.lstm_backward1 = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.lstm_backward2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n","\n","        # Linear layer for mode 1\n","        self.linear_mode1 = nn.Linear(200, vocab_size)\n","\n","        # Linear layer for mode 2\n","        self.linear_mode2 = nn.Linear(200, vocab_size)\n","\n","        # Define trainable weights for mode 0 as a vector of size 3 with values 0.33 each\n","        self.weights_mode0 = nn.Parameter(torch.tensor([0.33, 0.33, 0.33], dtype=torch.float32))\n","\n","        self.final_linear = nn.Linear(self.max_len * self.embedding_dim * 2, num_classes)\n","\n","    def forward(self, input_data, mode):\n","        if mode == 0:\n","\n","            char_input = input_data  # Input is now character-level data\n","\n","            # Use char_cnn_output for further processing\n","            forward_embed = self.char_cnn1(char_input)\n","\n","            # Use char_cnn_output for further processing\n","            backward_embed = self.char_cnn2(char_input)\n","\n","\n","            # Forward Language Model 1\n","            forward_lstm1, _ = self.lstm_forward1(forward_embed)\n","            \n","            # Backward Language Model 1\n","            backward_lstm1, _ = self.lstm_backward1(backward_embed)\n","            \n","            # Concatenate forward1 and backward1 outputs\n","            lstm_concat1 = torch.cat((forward_lstm1, backward_lstm1), dim=-1)\n","            \n","            # Forward Language Model 2\n","            forward_lstm2, _ = self.lstm_forward2(forward_lstm1)\n","            \n","            # Backward Language Model 2\n","            backward_lstm2, _ = self.lstm_backward2(backward_lstm1)\n","            \n","            # Concatenate forward2 and backward2 outputs\n","            lstm_concat2 = torch.cat((forward_lstm2, backward_lstm2), dim=-1)\n","\n","            # Concatenate forward and backward embeddings word by word\n","            concatenated_embeddings = torch.cat((forward_embed, backward_embed), dim=-1)\n","            \n","            # Apply trainable weights for mode 0 to the concatenated tensors\n","            weighted_embeddings = concatenated_embeddings * self.weights_mode0[0]\n","            weighted_lstm1 = lstm_concat1 * self.weights_mode0[1]\n","            weighted_lstm2 = lstm_concat2 * self.weights_mode0[2]\n","\n","            # Sum along the last dimension (600) to get a (32 x 256 x 200) tensor\n","            summed_tensor = weighted_embeddings + weighted_lstm1 + weighted_lstm2\n","\n","            # Reshape the output to 32 x (256 x vocabSize)\n","            output = summed_tensor.view(-1, self.max_len * self.embedding_dim *2)\n","\n","            # Additional linear layer to reduce the output to 32 x 4\n","            output = self.final_linear(output)\n","\n","            # output = F.softmax(output, dim=1)\n","\n","            return output\n","\n","        elif mode == 1:\n","            char_input = input_data  # Input is now character-level data\n","\n","            # Use char_cnn_output for further processing\n","            forward_embed = self.char_cnn1(char_input)\n","\n","\n","            # Forward Language Model 1\n","            forward_lstm1, _ = self.lstm_forward1(forward_embed, None) # Set batch_first to False\n","\n","            # Forward Language Model 2\n","            forward_lstm2, _ = self.lstm_forward2(forward_lstm1, None) # Set batch_first to False\n","\n","            # Concatenate forward_lstm1 and forward_lstm2 outputs\n","            lstm_concat = torch.cat((forward_lstm1, forward_lstm2), dim=-1)\n","\n","            # Apply linear layer for mode 1\n","            output = self.linear_mode1(lstm_concat)\n","\n","            return output\n","        \n","        elif mode == 2:\n","            char_input = input_data  # Input is now character-level data\n","\n","            # Use char_cnn_output for further processing\n","            backward_embed = self.char_cnn2(char_input)\n","            \n","            # Backward Language Model 1\n","            backward_lstm1, _ = self.lstm_backward1(backward_embed, None) # Set batch_first to False\n","\n","            # Backward Language Model 2\n","            backward_lstm2, _ = self.lstm_backward2(backward_lstm1, None) # Set batch_first to False\n","\n","            # Concatenate backward_lstm1 and backward_lstm2 outputs\n","            lstm_concat = torch.cat((backward_lstm1, backward_lstm2), dim=-1)\n","\n","            # Apply linear layer for mode 2\n","            output = self.linear_mode2(lstm_concat)\n","\n","            return output\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"text/plain":["ELMo(\n","  (embedding1): Embedding(81098, 100)\n","  (embedding2): Embedding(81098, 100)\n","  (char_cnn1): CharCNN(\n","    (char_embedding): Embedding(2, 50)\n","    (conv_layers): ModuleList(\n","      (0): Conv1d(50, 100, kernel_size=(3,), stride=(1,))\n","      (1): Conv1d(50, 100, kernel_size=(4,), stride=(1,))\n","      (2): Conv1d(50, 100, kernel_size=(5,), stride=(1,))\n","    )\n","  )\n","  (lstm_forward1): LSTM(100, 100, batch_first=True)\n","  (lstm_forward2): LSTM(100, 100, batch_first=True)\n","  (lstm_backward1): LSTM(100, 100, batch_first=True)\n","  (lstm_backward2): LSTM(100, 100, batch_first=True)\n","  (linear_mode1): Linear(in_features=200, out_features=81098, bias=True)\n","  (linear_mode2): Linear(in_features=200, out_features=81098, bias=True)\n","  (final_linear): Linear(in_features=51200, out_features=4, bias=True)\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["vocab_size = len(vocab)\n","embedding_dim = 100\n","hidden_dim = 100\n","max_len = 256\n","num_classes = 4\n","\n","num_filters = 100\n","kernel_sizes = [3, 4, 5]  # You can adjust these values based on experimentation\n","\n","# Define the character-level CNN\n","char_cnn1 = CharCNN(char_vocab_size, char_embedding_dim, num_filters, kernel_sizes)\n","char_cnn2 = CharCNN(char_vocab_size, char_embedding_dim, num_filters, kernel_sizes)\n","\n","# Define the ELMo model\n","elmo = ELMo(vocab_size, embedding_dim, hidden_dim, batch_size, max_len, embedding_matrix, char_cnn1=char_cnn1, char_cnn2=char_cnn2, use_char_cnn=True, num_classes=num_classes)\n","\n","# Move the model to the appropriate device (e.g., GPU)\n","elmo.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(elmo.parameters(), lr=0.001)  # Adjust the learning rate as needed\n","\n","# Training loop for mode 1 (next word prediction)\n","def train_mode1(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    total_tokens = 0  # Total tokens processed in the dataset\n","\n","    for inputs in tqdm(train_loader):\n","        inputs = inputs[0]\n","        inputs = inputs.to(device)\n","        optimizer.zero_grad()\n","\n","        # Use the first 5 words as input and the last 5 words as expected output\n","        input_seq = inputs[:, :5]\n","        target_seq = inputs[:, 1:]\n","\n","        # Forward pass\n","        outputs = model(input_seq, mode=1)\n","\n","        # Calculate the loss\n","        loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss\n","        total_loss += loss.item()\n","\n","        # Calculate the total tokens processed\n","        total_tokens += target_seq.numel()\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","    # Calculate perplexity based on the total loss and tokens\n","    perplexity = math.exp(total_loss / total_tokens)\n","\n","    avg_loss = total_loss / total_tokens\n","\n","    return avg_loss\n","\n","# Development loop\n","def eval_mode1(model, dev_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_tokens = 0  # Total tokens processed in the dataset\n","\n","    with torch.no_grad():\n","        for inputs in tqdm(dev_loader):\n","            inputs = inputs[0]\n","            inputs = inputs.to(device)\n","\n","            # Use the first 5 words as input and the last 5 words as expected output\n","            input_seq = inputs[:, :5]\n","            target_seq = inputs[:, 1:]\n","\n","            # Forward pass\n","            outputs = model(input_seq, mode=1)\n","\n","            # Calculate the loss\n","            loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss\n","            total_loss += loss.item()\n","\n","            # Calculate the total tokens processed\n","            total_tokens += target_seq.numel()\n","\n","    avg_loss = total_loss / total_tokens\n","\n","    return avg_loss\n","\n","# Testing loop\n","def test_mode1(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_tokens = 0  # Total tokens processed in the dataset\n","\n","    with torch.no_grad():\n","        for inputs in tqdm(test_loader):\n","            inputs = inputs[0]\n","            inputs = inputs.to(device)\n","\n","            # Use the first 5 words as input and the last 5 words as expected output\n","            input_seq = inputs[:, :5]\n","            target_seq = inputs[:, 1:]\n","\n","            # Forward pass\n","            outputs = model(input_seq, mode=1)\n","\n","            # Calculate the loss\n","            loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss\n","            total_loss += loss.item()\n","\n","            # Calculate the total tokens processed\n","            total_tokens += target_seq.numel()\n","\n","    avg_loss = total_loss / total_tokens\n","\n","    return avg_loss\n","\n","# Training loop for mode 2 (previous word prediction)\n","def train_mode2(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    total_tokens = 0  # Total tokens processed in the dataset\n","\n","    for inputs in tqdm(train_loader):\n","        inputs = inputs[0]\n","        inputs = inputs.to(device)\n","        optimizer.zero_grad()\n","\n","        # Use the first 5 words as input and the last 5 words as expected output\n","        input_seq = inputs[:, :5]\n","        target_seq = inputs[:, 1:]\n","\n","        # Forward pass\n","        outputs = model(input_seq, mode=2)\n","\n","        # Calculate the loss\n","        loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss\n","        total_loss += loss.item()\n","\n","        # Calculate the total tokens processed\n","        total_tokens += target_seq.numel()\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","    avg_loss = total_loss / total_tokens\n","\n","    return avg_loss\n","\n","# Development loop\n","def eval_mode2(model, dev_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_tokens = 0  # Total tokens processed in the dataset\n","\n","    with torch.no_grad():\n","        for inputs in tqdm(dev_loader):\n","            inputs = inputs[0]\n","            inputs = inputs.to(device)\n","\n","            # Use the first 5 words as input and the last 5 words as expected output\n","            input_seq = inputs[:, :5]\n","            target_seq = inputs[:, 1:]\n","\n","            # Forward pass\n","            outputs = model(input_seq, mode=2)\n","\n","            # Calculate the loss\n","            loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss\n","            total_loss += loss.item()\n","\n","            # Calculate the total tokens processed\n","            total_tokens += target_seq.numel()\n","\n","    avg_loss = total_loss / total_tokens\n","\n","    return avg_loss\n","\n","# Testing loop\n","def test_mode2(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_tokens = 0  # Total tokens processed in the dataset\n","\n","    with torch.no_grad():\n","        for inputs in tqdm(test_loader):\n","            inputs = inputs[0]\n","            inputs = inputs.to(device)\n","\n","            # Use the first 5 words as input and the last 5 words as expected output\n","            input_seq = inputs[:, :5]\n","            target_seq = inputs[:, 1:]\n","\n","            # Forward pass\n","            outputs = model(input_seq, mode=2)\n","\n","            # Calculate the loss\n","            loss = criterion(outputs.permute(0, 2, 1), target_seq)  # Permute for CrossEntropyLoss\n","            total_loss += loss.item()\n","\n","            # Calculate the total tokens processed\n","            total_tokens += target_seq.numel()\n","\n","    avg_loss = total_loss / total_tokens\n","\n","    return avg_loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training hyperparameters\n","num_epochs = 5  # Adjust as needed\n","\n","# Initialize empty lists to store loss and perplexity values\n","loss_mode1_values = []\n","perplexity_mode1_values = []\n","loss_mode2_values = []\n","perplexity_mode2_values = []\n","dev_loss_mode1_values = []  # To store dev loss\n","dev_perplexity_mode1_values = []  # To store dev perplexity\n","dev_loss_mode2_values = []  # To store dev loss\n","dev_perplexity_mode2_values = []  # To store dev perplexity\n","test_loss_mode1_values = []  # To store test loss\n","test_perplexity_mode1_values = []  # To store test perplexity\n","test_loss_mode2_values = []  # To store test loss\n","test_perplexity_mode2_values = []  # To store test perplexity\n","\n","# Training loop for mode 1\n","for epoch in range(num_epochs):\n","    train_loss_mode1 = train_mode1(elmo, train_loader, optimizer, criterion, device)\n","    loss_mode1_values.append(train_loss_mode1)\n","\n","    # Calculate perplexity based on the loss (assuming cross-entropy loss)\n","    perplexity_mode1 = math.exp(train_loss_mode1)\n","    perplexity_mode1_values.append(perplexity_mode1)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} (Mode 1) - Train Loss: {train_loss_mode1:.4f}, Train Perplexity: {perplexity_mode1:.4f}\")\n","\n","    # Evaluate on dev dataset\n","    dev_loss_mode1 = eval_mode1(elmo, dev_loader, criterion, device)\n","    dev_loss_mode1_values.append(dev_loss_mode1)\n","\n","    # Calculate perplexity based on the dev loss (assuming cross-entropy loss)\n","    dev_perplexity_mode1 = math.exp(dev_loss_mode1)\n","    dev_perplexity_mode1_values.append(dev_perplexity_mode1)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} (Mode 1) - Dev Loss: {dev_loss_mode1:.4f}, Dev Perplexity: {dev_perplexity_mode1:.4f}\")\n","\n","# Training loop for mode 2\n","for epoch in range(num_epochs):\n","    train_loss_mode2 = train_mode2(elmo, train_loader, optimizer, criterion, device)\n","    loss_mode2_values.append(train_loss_mode2)\n","\n","    # Calculate perplexity based on the loss (assuming cross-entropy loss)\n","    perplexity_mode2 = math.exp(train_loss_mode2)\n","    perplexity_mode2_values.append(perplexity_mode2)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} (Mode 2) - Train Loss: {train_loss_mode2:.4f}, Train Perplexity: {perplexity_mode2:.4f}\")\n","\n","    # Evaluate on dev dataset\n","    dev_loss_mode2 = eval_mode2(elmo, dev_loader, criterion, device)\n","    dev_loss_mode2_values.append(dev_loss_mode2)\n","\n","    # Calculate perplexity based on the dev loss (assuming cross-entropy loss)\n","    dev_perplexity_mode2 = math.exp(dev_loss_mode2)\n","    dev_perplexity_mode2_values.append(dev_perplexity_mode2)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} (Mode 2) - Dev Loss: {dev_loss_mode2:.4f}, Dev Perplexity: {dev_perplexity_mode2:.4f}\")\n","\n","# Testing loop\n","test_loss_mode1 = test_mode1(elmo, test_loader, criterion, device)\n","test_loss_mode2 = test_mode2(elmo, test_loader, criterion, device)\n","\n","# Calculate perplexity based on the test loss (assuming cross-entropy loss)\n","test_perplexity_mode1 = math.exp(test_loss_mode1)\n","test_perplexity_mode2 = math.exp(test_loss_mode2)\n","\n","# Store test results\n","test_loss_mode1_values.append(test_loss_mode1)\n","test_perplexity_mode1_values.append(test_perplexity_mode1)\n","test_loss_mode2_values.append(test_loss_mode2)\n","test_perplexity_mode2_values.append(test_perplexity_mode2)\n","\n","print(f\"Test Loss (Mode 1): {test_loss_mode1:.4f}, Test Perplexity (Mode 1): {test_perplexity_mode1:.4f}\")\n","print(f\"Test Loss (Mode 2): {test_loss_mode2:.4f}, Test Perplexity (Mode 2): {test_perplexity_mode2:.4f}\")\n","\n","# Plotting loss and perplexity graphs\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 6))\n","\n","# Loss plot for Mode 1\n","plt.subplot(2, 2, 1)\n","plt.plot(range(num_epochs), loss_mode1_values, label=\"Train Loss (Mode 1)\")\n","plt.plot(range(num_epochs), dev_loss_mode1_values, label=\"Dev Loss (Mode 1)\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss (Mode 1)\")\n","plt.legend()\n","\n","# Perplexity plot for Mode 1\n","plt.subplot(2, 2, 2)\n","plt.plot(range(num_epochs), perplexity_mode1_values, label=\"Train Perplexity (Mode 1)\")\n","plt.plot(range(num_epochs), dev_perplexity_mode1_values, label=\"Dev Perplexity (Mode 1)\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Perplexity\")\n","plt.title(\"Perplexity (Mode 1)\")\n","plt.legend()\n","\n","# Loss plot for Mode 2\n","plt.subplot(2, 2, 3)\n","plt.plot(range(num_epochs), loss_mode2_values, label=\"Train Loss (Mode 2)\")\n","plt.plot(range(num_epochs), dev_loss_mode2_values, label=\"Dev Loss (Mode 2)\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Loss (Mode 2)\")\n","plt.legend()\n","\n","# Perplexity plot for Mode 2\n","plt.subplot(2, 2, 4)\n","plt.plot(range(num_epochs), perplexity_mode2_values, label=\"Train Perplexity (Mode 2)\")\n","plt.plot(range(num_epochs), dev_perplexity_mode2_values, label=\"Dev Perplexity (Mode 2)\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Perplexity\")\n","plt.title(\"Perplexity (Mode 2)\")\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Display test results\n","print(f\"Test Loss (Mode 1): {test_loss_mode1:.4f}, Test Perplexity (Mode 1): {test_perplexity_mode1:.4f}\")\n","print(f\"Test Loss (Mode 2): {test_loss_mode2:.4f}, Test Perplexity (Mode 2): {test_perplexity_mode2:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment Below (with Top K Accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import torch\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# from tqdm import tqdm\n","\n","# # Define the loss function and optimizer\n","# criterion = nn.CrossEntropyLoss()\n","# optimizer = optim.Adam(elmo.parameters(), lr=0.001)  # Adjust the learning rate as needed\n","\n","# # Training loop for mode 1 (next word prediction)\n","# def train_mode1(model, train_loader, optimizer, criterion, device):\n","#     model.train()\n","#     total_loss = 0.0\n","#     total_correct = 0\n","\n","#     for inputs in tqdm(train_loader):\n","#         inputs = inputs[0]\n","#         inputs = inputs.to(device)\n","#         optimizer.zero_grad()\n","\n","#         # Use the first 5 words as input and the last 5 words as expected output\n","#         input_seq = inputs[:, :5]\n","#         target_seq = inputs[:, 1:]\n","\n","#         # Forward pass\n","#         outputs = model(input_seq, mode=1)\n","\n","#         # Calculate the values for the one-hot encoding\n","#         value_for_correct_idx = 0.1\n","#         value_for_other_indices = 0.9 / model.vocab_size\n","\n","#         # Create the one-hot encoding tensor\n","#         target_one_hot = torch.full((target_seq.size(0), target_seq.size(1), model.vocab_size), value_for_other_indices).to(device)\n","\n","#         # Set the correct indices to the specified value\n","#         target_one_hot.scatter_(2, target_seq.unsqueeze(2), value_for_correct_idx)\n","\n","#         # Calculate the loss\n","#         loss = criterion(outputs, target_one_hot)\n","#         total_loss += loss.item()\n","\n","#         # Calculate top-k accuracy\n","#         # predictions = outputs.view(inputs.size(0), inputs.size(1), model.vocab_size)\n","#         predictions = outputs\n","#         _, top_indices = torch.topk(predictions, 50, dim=2)  # Get the top-10 predicted indices\n","#         correct_predictions = torch.any(top_indices == target_seq.unsqueeze(2), dim=2).sum().item()\n","#         total_correct += correct_predictions\n","\n","#         # Backward pass and optimization\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#     accuracy = total_correct / (len(train_loader.dataset) * 5)  # Calculate accuracy based on the entire dataset\n","#     return total_loss / len(train_loader), accuracy\n","\n","# # Training loop for mode 2 (previous word prediction)\n","# def train_mode2(model, train_loader, optimizer, criterion, device):\n","#     model.train()\n","#     total_loss = 0.0\n","#     total_correct = 0\n","\n","#     for inputs in tqdm(train_loader):\n","#         inputs = inputs[0]\n","#         inputs = inputs.to(device)\n","#         optimizer.zero_grad()\n","\n","#         # Use the first 5 words as input and the last 5 words as expected output\n","#         input_seq = inputs[:, :5]\n","#         target_seq = inputs[:, 1:]\n","\n","#         # Forward pass\n","#         outputs = model(input_seq, mode=2)\n","\n","#         # Calculate the values for the one-hot encoding\n","#         value_for_correct_idx = 0.5\n","#         value_for_other_indices = 0.5 / model.vocab_size\n","\n","#         # Create the one-hot encoding tensor\n","#         target_one_hot = torch.full((target_seq.size(0), target_seq.size(1), model.vocab_size), value_for_other_indices).to(device)\n","\n","#         # Set the correct indices to the specified value\n","#         target_one_hot.scatter_(2, target_seq.unsqueeze(2), value_for_correct_idx)\n","\n","#         # Calculate the loss\n","#         loss = criterion(outputs, target_one_hot)\n","#         total_loss += loss.item()\n","\n","#         # Calculate top-k accuracy\n","#         # predictions = outputs.view(inputs.size(0), inputs.size(1), model.vocab_size)\n","#         predictions = outputs\n","#         _, top_indices = torch.topk(predictions, 50, dim=2)  # Get the top-10 predicted indices\n","#         correct_predictions = torch.any(top_indices == target_seq.unsqueeze(2), dim=2).sum().item()\n","#         total_correct += correct_predictions\n","\n","#         # Backward pass and optimization\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#     accuracy = total_correct / (len(train_loader.dataset) * 5)  # Calculate accuracy based on the entire dataset\n","#     return total_loss / len(train_loader), accuracy\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Set the device (CPU or GPU)\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# elmo.to(device)\n","\n","# # Training hyperparameters\n","# num_epochs = 10  # Adjust as needed\n","\n","# # Initialize empty lists to store accuracy values\n","# acc1_values = []\n","# acc2_values = []\n","\n","# # Training loop for mode 1\n","# for epoch in range(num_epochs):\n","#     train_loss_mode1, acc1 = train_mode1(elmo, train_loader, optimizer, criterion, device)\n","#     acc1_values.append(acc1)\n","#     print(f\"Epoch {epoch+1}/{num_epochs} (Mode 1) - Loss: {train_loss_mode1:.4f} , Accuracy = {acc1}\")\n","\n","# # Training loop for mode 2\n","# for epoch in range(num_epochs):\n","#     train_loss_mode2, acc2 = train_mode2(elmo, train_loader, optimizer, criterion, device)\n","#     acc2_values.append(acc2) \n","#     print(f\"Epoch {epoch+1}/{num_epochs} (Mode 2) - Loss: {train_loss_mode2:.4f} , Accuracy = {acc2}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-23T06:34:06.258156Z","iopub.status.busy":"2023-04-23T06:34:06.257268Z","iopub.status.idle":"2023-04-23T06:34:06.443885Z","shell.execute_reply":"2023-04-23T06:34:06.442799Z","shell.execute_reply.started":"2023-04-23T06:34:06.258106Z"},"trusted":true},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","\n","# # Assuming you have recorded the accuracy values for each epoch in lists acc1 and acc2\n","# # acc1 contains accuracy values for Mode 1, and acc2 for Mode 2\n","\n","# epochs = range(1, num_epochs+1)\n","\n","# # Plot for Mode 1\n","# plt.figure(figsize=(10, 5))\n","# plt.plot(epochs, acc1_values, label='Mode 1', marker='o', linestyle='-')\n","# plt.title('Epoch vs. Accuracy (Mode 1)')\n","# plt.xlabel('Epoch')\n","# plt.ylabel('Accuracy')\n","# plt.legend()\n","# plt.grid(True)\n","# plt.show()\n","\n","# # Plot for Mode 2\n","# plt.figure(figsize=(10, 5))\n","# plt.plot(epochs, acc2_values, label='Mode 2', marker='o', linestyle='-')\n","# plt.title('Epoch vs. Accuracy (Mode 2)')\n","# plt.xlabel('Epoch')\n","# plt.ylabel('Accuracy')\n","# plt.legend()\n","# plt.grid(True)\n","# plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment Above"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convert n-grams to PyTorch tensors\n","train_data_elmo = torch.tensor(train_indices_elmo, dtype=torch.long)\n","dev_data_elmo = torch.tensor(dev_indices_elmo, dtype=torch.long)\n","test_data_elmo = torch.tensor(test_indices_elmo, dtype=torch.long)\n","\n","# Combine data with labels\n","train_data_with_labels = TensorDataset(train_data_elmo[:10000], torch.tensor(train_labels[:10000], dtype=torch.long))\n","dev_data_with_labels = TensorDataset(dev_data_elmo[:500], torch.tensor(dev_labels[:500], dtype=torch.long))\n","test_data_with_labels = TensorDataset(test_data_elmo[:500], torch.tensor(test_labels[:500], dtype=torch.long))\n","\n","# Create DataLoader instances\n","batch_size = 32\n","\n","train_loader = DataLoader(train_data_with_labels, batch_size=batch_size, shuffle=False)\n","dev_loader = DataLoader(dev_data_with_labels, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_data_with_labels, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display the first few sentences and labels for train, dev, and test sets in index form\n","print(\"Train Set Sentences (Indices):\")\n","print(train_indices_elmo[:2])  # Displaying the first 2 sentences for brevity\n","print(\"Train Set Labels:\")\n","print(train_labels[:2])\n","\n","print(\"\\nDevelopment Set Sentences (Indices):\")\n","print(dev_indices_elmo[:2])\n","print(\"Development Set Labels:\")\n","print(dev_labels[:2])\n","\n","print(\"\\nTest Set Sentences (Indices):\")\n","print(test_indices_elmo[:2])\n","print(\"Test Set Labels:\")\n","print(test_labels[:2])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training loop for mode 0 (4-class classification) on GPU\n","def train_elmo(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0.0\n","    total_correct = 0\n","    all_predicted = []\n","    all_actual = []\n","\n","    for inputs, target_seq in tqdm(train_loader):\n","        inputs = inputs.to(device)\n","        target_seq = target_seq.to(device)\n","        optimizer.zero_grad()\n","\n","        input_seq = inputs\n","\n","        # Forward pass\n","        outputs = model(input_seq, mode=0)\n","\n","        target_seq = target_seq - 1\n","\n","        # Calculate the loss directly with integer class labels\n","        loss = criterion(outputs, target_seq)\n","        total_loss += loss.item()\n","\n","        # Calculate correct predictions\n","        predicted_classes = torch.argmax(outputs, dim=1)\n","        correct_predictions = torch.sum(torch.eq(predicted_classes, target_seq))\n","        total_correct += correct_predictions.item()\n","\n","        all_predicted.extend(predicted_classes.cpu().numpy())\n","        all_actual.extend(target_seq.cpu().numpy())\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","    accuracy = total_correct / len(all_actual)  # Calculate accuracy based on the entire dataset\n","\n","    return total_loss / len(train_loader), accuracy, all_predicted, all_actual\n","\n","# Evaluation loop for mode 0 (4-class classification) on GPU\n","def evaluate_elmo(model, dev_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    all_predicted = []\n","    all_actual = []\n","\n","    with torch.no_grad():\n","        for inputs, target_seq in tqdm(dev_loader):\n","            inputs = inputs.to(device)\n","            target_seq = target_seq.to(device)\n","\n","            input_seq = inputs\n","\n","            # Forward pass\n","            outputs = model(input_seq, mode=0)\n","\n","            target_seq = target_seq - 1\n","\n","            # Calculate the loss directly with integer class labels\n","            loss = criterion(outputs, target_seq)\n","            total_loss += loss.item()\n","\n","            # Calculate correct predictions\n","            predicted_classes = torch.argmax(outputs, dim=1)\n","            correct_predictions = torch.sum(torch.eq(predicted_classes, target_seq))\n","            total_correct += correct_predictions.item()\n","\n","            all_predicted.extend(predicted_classes.cpu().numpy())\n","            all_actual.extend(target_seq.cpu().numpy())\n","\n","    accuracy = total_correct / len(all_actual)  # Calculate accuracy based on the entire dataset\n","\n","    return total_loss / len(dev_loader), accuracy, all_predicted, all_actual\n","\n","\n","# Test loop for mode 0 (4-class classification) on GPU\n","def evaluate_elmo(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0.0\n","    total_correct = 0\n","    all_predicted = []\n","    all_actual = []\n","\n","    with torch.no_grad():\n","        for inputs, target_seq in tqdm(test_loader):\n","            inputs = inputs.to(device)\n","            target_seq = target_seq.to(device)\n","\n","            input_seq = inputs\n","\n","            # Forward pass\n","            outputs = model(input_seq, mode=0)\n","\n","            target_seq = target_seq - 1\n","\n","            # Calculate the loss directly with integer class labels\n","            loss = criterion(outputs, target_seq)\n","            total_loss += loss.item()\n","\n","            # Calculate correct predictions\n","            predicted_classes = torch.argmax(outputs, dim=1)\n","            correct_predictions = torch.sum(torch.eq(predicted_classes, target_seq))\n","            total_correct += correct_predictions.item()\n","\n","            all_predicted.extend(predicted_classes.cpu().numpy())\n","            all_actual.extend(target_seq.cpu().numpy())\n","\n","    accuracy = total_correct / len(all_actual)  # Calculate accuracy based on the entire dataset\n","\n","    return total_loss / len(test_loader), accuracy, all_predicted, all_actual"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Training hyperparameters\n","# num_epochs = 5\n","\n","# # Initialize lists to store evaluation metrics\n","# accuracy_values = []\n","# precision_values = []\n","# recall_values = []\n","# f1_values = []\n","\n","# # Initialize variables to store predicted and actual values\n","# all_predicted = []\n","# all_actual = []\n","\n","# # Define the loss function and optimizer\n","# # criterion = nn.BCEWithLogitsLoss()\n","# criterion = nn.CrossEntropyLoss()\n","# optimizer = optim.Adam(elmo.parameters(), lr=0.001)\n","\n","\n","# # Training loop for mode 0\n","# for epoch in range(num_epochs):\n","#     loss, accuracy, predicted, actual = train_elmo(elmo, train_loader, optimizer, criterion, device)\n","\n","#     # Calculate precision, recall, and F1 score\n","#     precision = precision_score(actual, predicted, average='weighted')\n","#     recall = recall_score(actual, predicted, average='weighted')\n","#     f1 = f1_score(actual, predicted, average='weighted')\n","\n","#     # Store evaluation metrics\n","#     accuracy_values.append(accuracy)\n","#     precision_values.append(precision)\n","#     recall_values.append(recall)\n","#     f1_values.append(f1)\n","\n","#     # Store predicted and actual values for confusion matrix\n","#     all_predicted.extend(predicted)\n","#     all_actual.extend(actual)\n","\n","#     print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}, Accuracy: {accuracy:.2%}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n","\n","#     # # Plot evaluation metrics after each epoch\n","#     # plt.figure(figsize=(12, 6))\n","\n","#     # # Accuracy plot\n","#     # plt.subplot(2, 2, 1)\n","#     # plt.plot(range(epoch + 1), accuracy_values, label=\"Accuracy\")\n","#     # plt.xlabel(\"Epoch\")\n","#     # plt.ylabel(\"Accuracy\")\n","#     # plt.title(\"Accuracy\")\n","#     # plt.legend()\n","\n","#     # # Precision plot\n","#     # plt.subplot(2, 2, 2)\n","#     # plt.plot(range(epoch + 1), precision_values, label=\"Precision\")\n","#     # plt.xlabel(\"Epoch\")\n","#     # plt.ylabel(\"Precision\")\n","#     # plt.title(\"Precision\")\n","#     # plt.legend()\n","\n","#     # # Recall plot\n","#     # plt.subplot(2, 2, 3)\n","#     # plt.plot(range(epoch + 1), recall_values, label=\"Recall\")\n","#     # plt.xlabel(\"Epoch\")\n","#     # plt.ylabel(\"Recall\")\n","#     # plt.title(\"Recall\")\n","#     # plt.legend()\n","\n","#     # # F1 Score plot\n","#     # plt.subplot(2, 2, 4)\n","#     # plt.plot(range(epoch + 1), f1_values, label=\"F1 Score\")\n","#     # plt.xlabel(\"Epoch\")\n","#     # plt.ylabel(\"F1 Score\")\n","#     # plt.title(\"F1 Score\")\n","#     # plt.legend()\n","\n","#     # plt.tight_layout()\n","#     # plt.show()\n","\n","#     # # Replace this line with the correct number of classes in your confusion matrix\n","#     # num_classes = 4  # Change this to the actual number of classes\n","\n","#     # # Display confusion matrix after each epoch\n","#     # conf_matrix = confusion_matrix(all_actual, all_predicted)\n","#     # plt.figure(figsize=(8, 6))\n","#     # plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n","#     # plt.title('Confusion Matrix')\n","#     # plt.colorbar()\n","#     # tick_marks = np.arange(num_classes)  # Use the correct number of classes here\n","#     # plt.xticks(tick_marks, range(1, num_classes + 1), rotation=45)  # Adjust the labels as needed\n","#     # plt.yticks(tick_marks, range(1, num_classes + 1))  # Adjust the labels as needed\n","#     # plt.xlabel('Predicted')\n","#     # plt.ylabel('Actual')\n","#     # plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training hyperparameters\n","num_epochs = 5\n","\n","# Initialize lists to store evaluation metrics for training, dev, and test\n","train_accuracy_values = []\n","train_precision_values = []\n","train_recall_values = []\n","train_micro_f1_values = []\n","\n","dev_accuracy_values = []\n","dev_precision_values = []\n","dev_recall_values = []\n","dev_micro_f1_values = []\n","\n","test_accuracy_values = []\n","test_precision_values = []\n","test_recall_values = []\n","test_micro_f1_values = []\n","\n","# Initialize variables to store predicted and actual values\n","train_all_predicted = []\n","train_all_actual = []\n","\n","dev_all_predicted = []\n","dev_all_actual = []\n","\n","test_all_predicted = []\n","test_all_actual = []\n","\n","# Define the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(elmo.parameters(), lr=0.001)\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    # Training\n","    train_loss, train_accuracy, train_predicted, train_actual = train_elmo(elmo, train_loader, optimizer, criterion, device)\n","\n","    # Calculate precision, recall, and micro F1 score for training\n","    train_precision = precision_score(train_actual, train_predicted, average='weighted')\n","    train_recall = recall_score(train_actual, train_predicted, average='weighted')\n","    train_micro_f1 = f1_score(train_actual, train_predicted, average='micro')\n","\n","    # Store training evaluation metrics\n","    train_accuracy_values.append(train_accuracy)\n","    train_precision_values.append(train_precision)\n","    train_recall_values.append(train_recall)\n","    train_micro_f1_values.append(train_micro_f1)\n","\n","    train_all_predicted.extend(train_predicted)\n","    train_all_actual.extend(train_actual)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} (Train) - Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2%}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, Micro F1 Score: {train_micro_f1:.4f}\")\n","\n","    # Validation on dev_loader\n","    dev_loss, dev_accuracy, dev_predicted, dev_actual = evaluate_elmo(elmo, dev_loader, criterion, device)\n","\n","    # Calculate precision, recall, and micro F1 score for dev_loader\n","    dev_precision = precision_score(dev_actual, dev_predicted, average='weighted')\n","    dev_recall = recall_score(dev_actual, dev_predicted, average='weighted')\n","    dev_micro_f1 = f1_score(dev_actual, dev_predicted, average='micro')\n","\n","    # Store dev evaluation metrics per epoch\n","    dev_accuracy_values.append(dev_accuracy)\n","    dev_precision_values.append(dev_precision)\n","    dev_recall_values.append(dev_recall)\n","    dev_micro_f1_values.append(dev_micro_f1)\n","\n","    dev_all_predicted.extend(dev_predicted)\n","    dev_all_actual.extend(dev_actual)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs} (Dev) - Loss: {dev_loss:.4f}, Accuracy: {dev_accuracy:.2%}, Precision: {dev_precision:.4f}, Recall: {dev_recall:.4f}, Micro F1 Score: {dev_micro_f1:.4f}\")\n","\n","# Testing on test_loader after training\n","test_loss, test_accuracy, test_predicted, test_actual = evaluate_elmo(elmo, test_loader, criterion, device)\n","\n","# Calculate precision, recall, and micro F1 score for test_loader\n","test_precision = precision_score(test_actual, test_predicted, average='weighted')\n","test_recall = recall_score(test_actual, test_predicted, average='weighted')\n","test_micro_f1 = f1_score(test_actual, test_predicted, average='micro')\n","\n","# Store test evaluation metrics\n","test_accuracy_values.append(test_accuracy)\n","test_precision_values.append(test_precision)\n","test_recall_values.append(test_recall)\n","test_micro_f1_values.append(test_micro_f1)\n","\n","test_all_predicted.extend(test_predicted)\n","test_all_actual.extend(test_actual)\n","\n","print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2%}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, Micro F1 Score: {test_micro_f1:.4f}\")\n","\n","# Plot evaluation metrics\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 18))\n","\n","# Accuracy plot\n","plt.subplot(3, 2, 1)\n","plt.plot(range(num_epochs), train_accuracy_values, label=\"Train Accuracy\")\n","plt.plot(range(num_epochs), dev_accuracy_values, label=\"Dev Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.title(\"Accuracy\")\n","plt.legend()\n","\n","# Precision plot\n","plt.subplot(3, 2, 2)\n","plt.plot(range(num_epochs), train_precision_values, label=\"Train Precision\")\n","plt.plot(range(num_epochs), dev_precision_values, label=\"Dev Precision\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Precision\")\n","plt.title(\"Precision\")\n","plt.legend()\n","\n","# Recall plot\n","plt.subplot(3, 2, 3)\n","plt.plot(range(num_epochs), train_recall_values, label=\"Train Recall\")\n","plt.plot(range(num_epochs), dev_recall_values, label=\"Dev Recall\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Recall\")\n","plt.title(\"Recall\")\n","plt.legend()\n","\n","# Micro F1 Score plot\n","plt.subplot(3, 2, 4)\n","plt.plot(range(num_epochs), train_micro_f1_values, label=\"Train Micro F1 Score\")\n","plt.plot(range(num_epochs), dev_micro_f1_values, label=\"Dev Micro F1 Score\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Micro F1 Score\")\n","plt.title(\"Micro F1 Score\")\n","plt.legend()\n","\n","# Confusion Matrix for Test Data\n","conf_matrix = confusion_matrix(test_all_actual, test_all_predicted)\n","plt.subplot(3, 2, 5)\n","plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n","plt.title('Test Confusion Matrix')\n","plt.colorbar()\n","tick_marks = np.arange(len(conf_matrix))\n","plt.xticks(tick_marks, range(1, 5), rotation=45)\n","plt.yticks(tick_marks, range(1, 5))\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
